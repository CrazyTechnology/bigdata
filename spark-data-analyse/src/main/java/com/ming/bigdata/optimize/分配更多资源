1.性能调优的王道是增加更多的资源。只有资源给分配的充足了，无论是时间和性能都能得到明显的提升。
    写完一个复杂的spark作业之后，进行性能调优的时候。首先要考虑给项目最优的资源去运行。
    只有当资源到达最大化的时候。我们才会考虑去做其他的一些优化
2.问题分配哪些资源，在哪里分配资源，为什么分配了这些资源之后性能会提升
    分配哪些资源：
        executor、cpu per executor、drive memory、memory pre executor
    在哪里分配这些资源：
        提交spark作业时，用的spark-shell脚本，里面调整对应的参数
        /usr/local/spark/bin/spark-submit\
        --class com.ming.bigdata.spark.xxx.
        --num-executor 3\  配置executor的数量
        --driver-memory 100m\配置driver的内存，影响不大
        --executor-memory 100m \ 配置每个executor的内存
        --executor-core  3 \配置每个executor的cpu core 数量
        /要运行的jar包位置
